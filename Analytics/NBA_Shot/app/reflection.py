import streamlit as st

def app():
    st.title("ğŸ§  Model Reflection & Improvement Roadmap")

    st.markdown("""
    This section provides a comprehensive diagnostic summary and targeted recommendations across all models evaluated. Each insight is grounded in empirical findings from grouped performance, calibration, residual analysis, and spatial overlays.
    """)

    # ------------------------------
    st.header("ğŸ” Shared Model Insights")
    st.markdown("""
    #### âœ… 1. Feature & Interaction Review
    - **Raw features to reintroduce or engineer:**
      - `TOUCH_TIME` (as raw + interaction)
      - `DRIBBLES`
      - `PERIOD`, `GAME_CLOCK`
      - `SHOT_TYPE` (2PT vs 3PT)
      - `SHOT_CLOCK` (binned or logged)
    
    - **New interaction ideas:**
      - `SHOT_DIST Ã— CLOSE_DEF_DIST`
      - `DRIBBLES Ã— SHOT_DIST`
      - `TOUCH_TIME Ã— DRIBBLES`

    **Next Step:** Visualize feature distributions and run MI / SHAP / logit coeffs.
    """)

    # ------------------------------
    st.header("ğŸ”— Logistic Regression â€” Next Steps")
    st.markdown("""
    #### ğŸ§ª Expand with PolynomialFeatures
    - Use `PolynomialFeatures(degree=2, interaction_only=True)` to auto-generate second-order terms.
    - Wrap in a pipeline: `StandardScaler â†’ Poly â†’ LogisticRegression`

    #### âš–ï¸ Regularization Search
    ```python
    {
        'penalty': ['l1', 'l2'],
        'C': [0.01, 0.1, 1, 10],
        'solver': ['liblinear', 'saga']
    }
    ```
    - Use `GridSearchCV` or `LogisticRegressionCV` with `neg_log_loss`.

    **Goal:** Improve flexibility without losing interpretability.
    """)

    # ------------------------------
    st.header("ğŸŒ² Random Forest â€” Next Steps")
    st.markdown("""
    #### ğŸ”§ Hyperparameter Tuning
    ```python
    {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 10, None],
        'min_samples_leaf': [1, 10, 50, 100],
        'max_features': ['sqrt', 'log2', None]
    }
    ```
    - Use `RandomizedSearchCV`, scoring by `neg_log_loss`

    #### ğŸ“Š Prune with Feature Importances
    - Extract `.feature_importances_`, drop lowest-ranked features
    - Retrain to improve generalization

    **Goal:** Tighten prediction variance, reduce coarse binning.
    """)

    # ------------------------------
    st.header("ğŸ“ˆ Summary: Model Behavior")
    st.markdown("""
    | Model     | Calibration     | Residual Spread | Shot Distance Bias | Notes                                 |
    |-----------|-----------------|------------------|---------------------|----------------------------------------|
    | logreg_v2 | Fair            | Symmetric, wide  | Strong underfit     | Best recall, poor rim resolution       |
    | rf_v3     | Good            | Bimodal, coarse  | Slight residual bias | Conservative, safe but imprecise      |
    | xgb_v2    | Excellent       | Tight, centered  | None                | Best spatial modeling + calibration    |

    **Diagnostic Takeaways:**
    - Logistic: Linear structure limits spatial flexibility.
    - RF: Improved with tuning, still prone to hard cutoffs.
    - XGB: Best overall performer â€” spatially intelligent, calibrated.
    """)

    # ------------------------------
    st.header("ğŸ€ Shot Context & Distance Insights")
    st.markdown("""
    #### ğŸ“Š Grouped Evaluation â€” Key Wins
    - **0â€“10 ft:** RF_v3 and XGB_v2 outperform others (rim touch modeling).
    - **Mid-range (10â€“25 ft):** All models converge â€” feature ceiling.
    - **Clutch, Contested, Mismatch:** XGB_v2 consistently wins.

    #### ğŸ“Œ Insight:
    XGB_v2 generalizes best in volatile contexts. RF_v3 is reliable but rigid. Logistic regression fails to separate complex scenarios.
    """)

    # ------------------------------
    st.header("ğŸ§ª Calibration & Residual Diagnostics")
    st.markdown("""
    - **Calibration:** XGB_v2 is closest to ideal. RF_v3 improved but still slightly underconfident at low scores.
    - **QQ Plots:**
      - Logistic: clear S-curve â†’ underfitting
      - RF: plateaus from vote-counting
      - XGB: closest to normality, best residual flow
    - **Residuals vs SHOT_DIST:**
      - LogReg â†’ strong upward slope (bias)
      - RF â†’ step-wise improvements, RF_v3 nearly flat
      - XGB â†’ flattest trend, lowest variance
    - **Residual Histograms:**
      - RF models are sharply bimodal (overconfident)
      - XGB_v2 is symmetrical and smooth

    âœ… Conclusion: XGB_v2 = Best fit, lowest variance, sharpest probability control.
    """)

    # ------------------------------
    st.header("ğŸ“ Spatial Intelligence (Court Heatmaps)")
    st.markdown("""
    | Model     | Spatial Smoothness | Rim Focus | Corner 3 Handling | Realism |
    |-----------|---------------------|-----------|--------------------|---------|
    | logreg_v2 | âš ï¸ Low              | Moderate  | Uniform            | Low     |
    | rf_v3     | Moderate            | High      | Patchy             | Moderate|
    | xgb_v2    | âœ… Best             | âœ… Best   | âœ… Realistic        | âœ… High |

    **Takeaway:**
    XGB_v2 understands the court. It predicts with fine-grain fidelity, distinguishes high-percentage areas, and mirrors actual make patterns.
    """)

    # ------------------------------
    st.header("ğŸ¯ Final Recommendation & Actions")
    st.markdown("""
    âœ… Promote `xgb_v2` as primary production model
    - Excellent fit, reliable calibration, best spatial expressiveness

    ğŸ”„ Continue refining `rf_v3`
    - Explore feature pruning, interaction engineering

    ğŸ“ Rebuild `logreg_v2`
    - Add nonlinearity, binned shot clock, and polynomial terms

    ğŸ“Œ Next Dev Steps:
    - Evaluate added features
    - Try 2D court-aware engineered variables
    - Expand grouped eval to include player or lineup context

    ğŸ§  Reflection complete. Use sidebar to explore diagnostics or trigger model updates.
    """)
