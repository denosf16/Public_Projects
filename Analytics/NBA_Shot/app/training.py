import streamlit as st
import pandas as pd
import plotly.express as px

def app():
    st.title("üß™ Model Training & Selection Overview")

    # --- Introduction ---
    st.markdown("""
    This page summarizes the model development process for predicting shot success (`SHOT_MADE`) using contextual and defender-related features.
    
    The modeling pipeline followed the supervised learning framework:
    > **Data Cleaning ‚Üí Feature Engineering ‚Üí Model Training ‚Üí Evaluation ‚Üí Deployment**
    
    We compare four model families trained offline:
    
    - **Logistic Regression (v2)**: A linear, interpretable baseline with interaction terms  
    - **Random Forest (v3)**: A bagging-based tree ensemble capturing nonlinearities and interactions  
    - **XGBoost (v1)**: A boosting model tuned on raw + interaction features  
    - **XGBoost (v2)**: A bin-weighted variant accounting for distance-based class imbalance  
    """)

    st.markdown("---")
    st.subheader("üìê Model Equations and Strategy")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        #### Logistic Regression (v2)
        Linear model with log-odds formulation:
        $$
        P(\\text{SHOT\\_MADE}=1) = \\frac{1}{1 + e^{- (\\beta_0 + \\sum \\beta_i X_i)}}
        $$
        - Includes manually engineered terms:
          - `SCxTT = SHOT_CLOCK √ó TOUCH_TIME`
          - `CDxHD = CLOSE_DEF_DIST √ó HEIGHT_DIFFERENTIAL`
        - Grid-searched over penalty type (`l1`, `l2`) and regularization strength (`C`)
        """)

    with col2:
        st.markdown("""
        #### Random Forest (v3)
        Ensemble of bootstrapped decision trees:
        $$
        P(\\text{SHOT\\_MADE}=1) = \\frac{1}{T} \\sum_{t=1}^{T} h_t(X)
        $$
        - Feature space included:
          - `TOUCH_TIME`, `HEIGHT_DIFFERENTIAL`, `CLOCK_TOUCH`, `HEIGHT_CLOSE`
        - Hyperparameters tuned:
          - `n_estimators`, `max_depth`, `min_samples_leaf`, `max_features`
        """)

    st.markdown("""
    #### XGBoost (v1 & v2)
    Gradient-boosted trees with additive updates:
    $$
    F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)
    $$
    - v1: Added `DIST_x_CLOSE_DEF` interaction  
    - v2: Applied one-hot encoding to `SHOT_DIST` bins + `DIST_x_DEF` interaction  
    - Used `class_weight='balanced'` by bin to mitigate distance imbalance  
    """)

    st.markdown("---")
    st.subheader("üìä Model Evaluation: Log Loss")

    try:
        results_df = pd.read_csv("output/model_results.csv")
        st.dataframe(results_df, use_container_width=True)

        fig = px.bar(
            results_df,
            x="Model", y="Log Loss", color="Model",
            title="Model Comparison: Log Loss (Lower = Better)",
            text_auto=".3f"
        )
        st.plotly_chart(fig, use_container_width=True)
    except FileNotFoundError:
        st.warning("‚ö†Ô∏è Model results not found. Please run training scripts.")

    st.markdown("---")
    st.subheader("üéõÔ∏è Hyperparameter Tuning Strategy")

    st.markdown("""
    All models were tuned using **GridSearchCV** with 5-fold cross-validation and `neg_log_loss` scoring.
    
    | Model          | Key Hyperparameters                             | Selection Basis                     |
    |----------------|--------------------------------------------------|-------------------------------------|
    | Logistic Reg.  | `C`, `penalty`, `solver`                         | Best out-of-sample log loss         |
    | Random Forest  | `n_estimators`, `max_depth`, `min_samples_leaf` | Balance of stability and flexibility |
    | XGBoost (v1/v2)| `learning_rate`, `depth`, `colsample_bytree`     | Best in class when tuned properly   |
    
    üìå Hyperparameters control **model capacity** and the **bias‚Äìvariance tradeoff**. Grid search ensures reproducibility and robustness (see `train_log.py`, `train_rf.py`, `train_xgbv2.py`).
    """)

    st.markdown("---")
    st.subheader("üß† Modeling Philosophy & Tradeoffs")

    st.markdown("""
    - **Logistic Regression**  
      ‚úÖ Interpretable  
      ‚úÖ Coefficients show directional influence  
      ‚ùå Cannot model nonlinear effects or interactions unless manually added
    
    - **Random Forest**  
      ‚úÖ Robust to overfitting with enough trees  
      ‚úÖ Captures nonlinear patterns + interactions automatically  
      ‚ùå Requires careful tuning (depth, leaf size)
    
    - **XGBoost**  
      ‚úÖ Best performance in many structured data tasks  
      ‚úÖ Additive corrections help learn subtle signal  
      ‚ùå Requires binning/weighting to handle imbalanced contexts
    
    **Why multiple models?**  
    Because interpretability, generalization, and performance aren‚Äôt always aligned.
    
    üëâ We retain all trained models and compare their behavior in the evaluation stage.
    """)

    st.markdown("---")
    st.subheader("üìå Pipeline Architecture")

    st.markdown("""
    ```
    [Raw Shot Logs]
          ‚Üì
    [Data Cleaning + Filtering]
          ‚Üì
    [Feature Engineering]
          ‚Üì
    [Model Training]
          ‚Üì
    [Evaluation & Diagnostics]
          ‚Üì
    [Court-Level Visualization]
    ```

    All models are trained offline and version-controlled (`logreg_model_v2.pkl`, `rf_model_v3.pkl`, `xgb_model_v2.pkl`) and are loaded into this app for evaluation only.
    """)

    st.success("Navigate to **Model Evaluation** to explore confusion matrices, ROC curves, calibration, and grouped shot context accuracy.")



