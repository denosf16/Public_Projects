{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc6c136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downcasting floats.\n",
      "✅ Saved: weekly_stats (134,470 rows, 53 columns)\n",
      "✅ Saved: seasonal_stats (15,102 rows, 58 columns)\n",
      "✅ Saved: seasonal_rosters (63,323 rows, 37 columns)\n",
      "✅ Saved: combine_data (8,649 rows, 18 columns)\n",
      "✅ Saved: draft_picks (6,640 rows, 36 columns)\n",
      "✅ Saved: team_info (36 rows, 16 columns)\n",
      "✅ Saved: draft_values (262 rows, 6 columns)\n",
      "✅ Saved: id_mappings (12,023 rows, 35 columns)\n"
     ]
    }
   ],
   "source": [
    "import nfl_data_py as nfl\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "base_dir = r\"C:\\Repos\\NFL_Draft\\data\"\n",
    "\n",
    "def save_parquet(df, folder_name, file_name):\n",
    "    \"\"\"Save a DataFrame to Parquet format with automatic type coercion and logging.\"\"\"\n",
    "    try:\n",
    "        # Create folder if needed\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Coerce object-type numeric columns to avoid Arrow errors\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"object\" and df[col].str.isnumeric().sum() > 0:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{file_name}.parquet\")\n",
    "        df.to_parquet(file_path, index=False)\n",
    "        print(f\"✅ Saved: {file_name} ({df.shape[0]:,} rows, {df.shape[1]} columns)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save {file_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Define range of seasons\n",
    "years = list(range(1999, 2025))\n",
    "\n",
    "# ----------- Main Extraction -----------\n",
    "\n",
    "try:\n",
    "    weekly_stats = nfl.import_weekly_data(years)\n",
    "    save_parquet(weekly_stats, \"weekly_data\", \"weekly_stats\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading weekly_stats:\", e)\n",
    "\n",
    "try:\n",
    "    seasonal_stats = nfl.import_seasonal_data(years)\n",
    "    save_parquet(seasonal_stats, \"seasonal_data\", \"seasonal_stats\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading seasonal_stats:\", e)\n",
    "\n",
    "try:\n",
    "    seasonal_rosters = nfl.import_seasonal_rosters(years)\n",
    "    save_parquet(seasonal_rosters, \"seasonal_rosters\", \"seasonal_rosters\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading seasonal_rosters:\", e)\n",
    "\n",
    "try:\n",
    "    combine_data = nfl.import_combine_data()\n",
    "    save_parquet(combine_data, \"combine_data\", \"combine_data\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading combine_data:\", e)\n",
    "\n",
    "try:\n",
    "    draft_picks = nfl.import_draft_picks(years)\n",
    "    save_parquet(draft_picks, \"draft_picks\", \"draft_picks\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading draft_picks:\", e)\n",
    "\n",
    "try:\n",
    "    team_info = nfl.import_team_desc()\n",
    "    save_parquet(team_info, \"team_info\", \"team_info\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading team_info:\", e)\n",
    "\n",
    "try:\n",
    "    draft_values = nfl.import_draft_values()\n",
    "    save_parquet(draft_values, \"draft_values\", \"draft_values\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading draft_values:\", e)\n",
    "\n",
    "try:\n",
    "    id_mappings = nfl.import_ids()\n",
    "    save_parquet(id_mappings, \"id_mappings\", \"id_mappings\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error downloading id_mappings:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb62fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Summary saved to parquet_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = r\"C:\\Repos\\NFL_Draft\\data\"\n",
    "\n",
    "def summarize_parquet(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    summary = pd.DataFrame({\n",
    "        \"column\": df.columns,\n",
    "        \"dtype\": [str(df[col].dtype) for col in df.columns],\n",
    "        \"nullable\": [df[col].isnull().any() for col in df.columns],\n",
    "        \"null_pct\": [df[col].isnull().mean() for col in df.columns],\n",
    "        \"unique_vals\": [df[col].nunique(dropna=True) for col in df.columns],\n",
    "        \"example\": [df[col].dropna().iloc[0] if not df[col].dropna().empty else None for col in df.columns],\n",
    "    })\n",
    "    summary.insert(0, \"table\", os.path.basename(path).replace(\".parquet\", \"\"))\n",
    "    return summary\n",
    "\n",
    "# Collect all .parquet files\n",
    "parquet_files = []\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            parquet_files.append(os.path.join(root, file))\n",
    "\n",
    "# Generate and export summary\n",
    "if parquet_files:\n",
    "    summaries = pd.concat([summarize_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "    summaries.to_csv(os.path.join(base_dir, \"parquet_summary.csv\"), index=False)\n",
    "    print(\"✅ Summary saved to parquet_summary.csv\")\n",
    "else:\n",
    "    print(\"⚠️ No .parquet files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa27c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ combine_data cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root to sys.path if running from subfolder ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import COMBINE_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_combine_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce numeric columns\n",
    "    for col in [\"draft_year\", \"draft_round\", \"draft_ovr\", \"jersey_number\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Season to nullable integer\n",
    "    if \"season\" in df.columns:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Clean categorical text columns\n",
    "    for col in [\"draft_team\", \"position\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "combine_path = os.path.join(COMBINE_DIR, \"combine_data.parquet\")\n",
    "combine_cleaned_path = os.path.join(COMBINE_DIR, \"combine_data_cleaned.parquet\")\n",
    "\n",
    "df_combine = pd.read_parquet(combine_path)\n",
    "df_combine_cleaned = clean_combine_data(df_combine)\n",
    "df_combine_cleaned.to_parquet(combine_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ combine_data cleaned and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e271ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ draft_picks cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root to sys.path if running from misc/ ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import DRAFT_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_draft_picks(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce integer-like floats to Int64\n",
    "    for col in [\"draft_round\", \"draft_pick\", \"overall_pick\", \"season\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Strip text fields\n",
    "    for col in [\"player_name\", \"position\", \"team\", \"college\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "draft_path = os.path.join(DRAFT_DIR, \"draft_picks.parquet\")\n",
    "draft_cleaned_path = os.path.join(DRAFT_DIR, \"draft_picks_cleaned.parquet\")\n",
    "\n",
    "df_draft = pd.read_parquet(draft_path)\n",
    "df_draft_cleaned = clean_draft_picks(df_draft)\n",
    "df_draft_cleaned.to_parquet(draft_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ draft_picks cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c70233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ draft_values cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import DRAFT_VALUE_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_draft_values(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Clean placeholder values\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Cast pick to Int64 if it's a float with integer values\n",
    "    if \"pick\" in df.columns:\n",
    "        df[\"pick\"] = pd.to_numeric(df[\"pick\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Ensure chart column is clean string\n",
    "    if \"chart\" in df.columns:\n",
    "        df[\"chart\"] = df[\"chart\"].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "values_path = os.path.join(DRAFT_VALUE_DIR, \"draft_values.parquet\")\n",
    "values_cleaned_path = os.path.join(DRAFT_VALUE_DIR, \"draft_values_cleaned.parquet\")\n",
    "\n",
    "df_values = pd.read_parquet(values_path)\n",
    "df_values_cleaned = clean_draft_values(df_values)\n",
    "df_values_cleaned.to_parquet(values_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ draft_values cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea8dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ id_mappings cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import ID_MAP_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_id_mappings(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder values\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Strip all string/object columns\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "id_path = os.path.join(ID_MAP_DIR, \"id_mappings.parquet\")\n",
    "id_cleaned_path = os.path.join(ID_MAP_DIR, \"id_mappings_cleaned.parquet\")\n",
    "\n",
    "df_ids = pd.read_parquet(id_path)\n",
    "df_ids_cleaned = clean_id_mappings(df_ids)\n",
    "df_ids_cleaned.to_parquet(id_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ id_mappings cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b518f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ seasonal_stats cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import SEASONAL_STATS_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_seasonal_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder values\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Convert float-to-int where appropriate\n",
    "    for col in df.select_dtypes(include=\"float64\").columns:\n",
    "        if df[col].dropna().apply(float.is_integer).all():\n",
    "            df[col] = df[col].astype(\"Int64\")\n",
    "\n",
    "    # Clean string columns\n",
    "    for col in [\"player_id\", \"player_name\", \"team\", \"position\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Coerce season to Int64\n",
    "    if \"season\" in df.columns:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "seasonal_path = os.path.join(SEASONAL_STATS_DIR, \"seasonal_stats.parquet\")\n",
    "seasonal_cleaned_path = os.path.join(SEASONAL_STATS_DIR, \"seasonal_stats_cleaned.parquet\")\n",
    "\n",
    "df_seasonal = pd.read_parquet(seasonal_path)\n",
    "df_seasonal_cleaned = clean_seasonal_data(df_seasonal)\n",
    "df_seasonal_cleaned.to_parquet(seasonal_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ seasonal_stats cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f38bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ seasonal_rosters cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import ROSTERS_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_seasonal_rosters(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce numeric columns\n",
    "    for col in [\"jersey_number\", \"height\", \"weight\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Parse birth_date safely\n",
    "    if \"birth_date\" in df.columns:\n",
    "        df[\"birth_date\"] = pd.to_datetime(df[\"birth_date\"], errors=\"coerce\")\n",
    "\n",
    "    # Clean categorical/text fields\n",
    "    for col in [\"player_name\", \"position\", \"team\", \"status\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Coerce season\n",
    "    if \"season\" in df.columns:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "rosters_path = os.path.join(ROSTERS_DIR, \"seasonal_rosters.parquet\")\n",
    "rosters_cleaned_path = os.path.join(ROSTERS_DIR, \"seasonal_rosters_cleaned.parquet\")\n",
    "\n",
    "df_rosters = pd.read_parquet(rosters_path)\n",
    "df_rosters_cleaned = clean_seasonal_rosters(df_rosters)\n",
    "df_rosters_cleaned.to_parquet(rosters_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ seasonal_rosters cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d43717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ team_info cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import TEAM_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_team_info(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Clean text fields\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Coerce numeric columns if applicable\n",
    "    for col in [\"season\", \"team_id\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "team_path = os.path.join(TEAM_DIR, \"team_info.parquet\")\n",
    "team_cleaned_path = os.path.join(TEAM_DIR, \"team_info_cleaned.parquet\")\n",
    "\n",
    "df_team = pd.read_parquet(team_path)\n",
    "df_team_cleaned = clean_team_info(df_team)\n",
    "df_team_cleaned.to_parquet(team_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ team_info cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b982843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ weekly_stats cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import WEEKLY_STATS_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_weekly_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce float-to-int where appropriate\n",
    "    for col in df.select_dtypes(include=\"float64\").columns:\n",
    "        if df[col].dropna().apply(float.is_integer).all():\n",
    "            df[col] = df[col].astype(\"Int64\")\n",
    "\n",
    "    # Ensure season/week are integers\n",
    "    for col in [\"season\", \"week\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Clean text columns\n",
    "    for col in [\"player_id\", \"player_name\", \"team\", \"opponent\", \"position\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "weekly_path = os.path.join(WEEKLY_STATS_DIR, \"weekly_stats.parquet\")\n",
    "weekly_cleaned_path = os.path.join(WEEKLY_STATS_DIR, \"weekly_stats_cleaned.parquet\")\n",
    "\n",
    "df_weekly = pd.read_parquet(weekly_path)\n",
    "df_weekly_cleaned = clean_weekly_data(df_weekly)\n",
    "df_weekly_cleaned.to_parquet(weekly_cleaned_path, index=False)\n",
    "\n",
    "print(\"✅ weekly_stats cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a17ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning 1999...\n",
      "🧹 Cleaning 2000...\n",
      "🧹 Cleaning 2001...\n",
      "🧹 Cleaning 2002...\n",
      "🧹 Cleaning 2003...\n",
      "🧹 Cleaning 2004...\n",
      "🧹 Cleaning 2005...\n",
      "🧹 Cleaning 2006...\n",
      "🧹 Cleaning 2007...\n",
      "🧹 Cleaning 2008...\n",
      "🧹 Cleaning 2009...\n",
      "🧹 Cleaning 2010...\n",
      "🧹 Cleaning 2011...\n",
      "🧹 Cleaning 2012...\n",
      "🧹 Cleaning 2013...\n",
      "🧹 Cleaning 2014...\n",
      "🧹 Cleaning 2015...\n",
      "🧹 Cleaning 2016...\n",
      "🧹 Cleaning 2017...\n",
      "🧹 Cleaning 2018...\n",
      "🧹 Cleaning 2019...\n",
      "🧹 Cleaning 2020...\n",
      "🧹 Cleaning 2021...\n",
      "🧹 Cleaning 2022...\n",
      "🧹 Cleaning 2023...\n",
      "🧹 Cleaning 2024...\n",
      "✅ All seasons cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replace this with your actual import if using config.py\n",
    "from config import PBP_DIR\n",
    "\n",
    "# Step 1: Gather all .parquet files\n",
    "pbp_files = sorted([\n",
    "    os.path.join(PBP_DIR, f)\n",
    "    for f in os.listdir(PBP_DIR)\n",
    "    if f.endswith(\".parquet\")\n",
    "])\n",
    "\n",
    "# Step 2: Build a unified column set\n",
    "column_union = set()\n",
    "for path in pbp_files:\n",
    "    df = pd.read_parquet(path)\n",
    "    column_union.update(df.columns)\n",
    "column_union = sorted(column_union)\n",
    "\n",
    "# Step 3: Define cleaner\n",
    "def clean_pbp(df):\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    df.replace([\"--\", \"N/A\", \"\", \"NaN\"], pd.NA, inplace=True)\n",
    "\n",
    "    for col in column_union:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    numeric_cols = [\"yards_gained\", \"epa\", \"down\", \"quarter\", \"game_seconds_remaining\"]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    for col in [\"game_id\", \"play_id\", \"posteam\", \"defteam\", \"play_type\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df[column_union]\n",
    "\n",
    "# Step 4: Clean and save each season\n",
    "cleaned_dfs = []\n",
    "for path in pbp_files:\n",
    "    season_year = os.path.basename(path).split(\"_\")[1].split(\".\")[0]\n",
    "    print(f\"🧹 Cleaning {season_year}...\")\n",
    "\n",
    "    df_raw = pd.read_parquet(path)\n",
    "    df_cleaned = clean_pbp(df_raw)\n",
    "    df_cleaned[\"season\"] = int(season_year)\n",
    "\n",
    "    cleaned_path = os.path.join(PBP_DIR, f\"pbp_{season_year}_cleaned.parquet\")\n",
    "    df_cleaned.to_parquet(cleaned_path, index=False)\n",
    "    cleaned_dfs.append(df_cleaned)\n",
    "\n",
    "# Step 5: Save full combined file (optional)\n",
    "df_all = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "df_all.to_parquet(os.path.join(PBP_DIR, \"pbp_all_cleaned.parquet\"), index=False)\n",
    "\n",
    "print(\"✅ All seasons cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e66bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SQL compatibility summary saved to: data/sql_compatibility_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from config import (\n",
    "    COMBINE_DIR, DRAFT_DIR, DRAFT_VALUE_DIR, ID_MAP_DIR,\n",
    "    SEASONAL_STATS_DIR, ROSTERS_DIR, TEAM_DIR, WEEKLY_STATS_DIR, PBP_DIR, DATA_DIR\n",
    ")\n",
    "\n",
    "# Cleaned files to check\n",
    "cleaned_files = {\n",
    "    \"combine_data\": os.path.join(COMBINE_DIR, \"combine_data_cleaned.parquet\"),\n",
    "    \"draft_picks\": os.path.join(DRAFT_DIR, \"draft_picks_cleaned.parquet\"),\n",
    "    \"draft_values\": os.path.join(DRAFT_VALUE_DIR, \"draft_values_cleaned.parquet\"),\n",
    "    \"id_mappings\": os.path.join(ID_MAP_DIR, \"id_mappings_cleaned.parquet\"),\n",
    "    \"seasonal_stats\": os.path.join(SEASONAL_STATS_DIR, \"seasonal_stats_cleaned.parquet\"),\n",
    "    \"seasonal_rosters\": os.path.join(ROSTERS_DIR, \"seasonal_rosters_cleaned.parquet\"),\n",
    "    \"team_info\": os.path.join(TEAM_DIR, \"team_info_cleaned.parquet\"),\n",
    "    \"weekly_stats\": os.path.join(WEEKLY_STATS_DIR, \"weekly_stats_cleaned.parquet\"),\n",
    "    \"pbp_all\": os.path.join(PBP_DIR, \"pbp_all_cleaned.parquet\")\n",
    "}\n",
    "\n",
    "# Run checks\n",
    "results = []\n",
    "\n",
    "for name, path in cleaned_files.items():\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_parquet(path)\n",
    "        row_count = df.shape[0]\n",
    "        col_count = df.shape[1]\n",
    "\n",
    "        for col in df.columns:\n",
    "            dtype = str(df[col].dtype)\n",
    "            null_pct = df[col].isnull().mean()\n",
    "            is_unique = df[col].is_unique\n",
    "            all_nonnull = df[col].notnull().all()\n",
    "            is_pk = is_unique and all_nonnull\n",
    "\n",
    "            max_len = (\n",
    "                df[col].astype(str).str.len().max()\n",
    "                if df[col].dtype == \"object\" else None\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"table\": name,\n",
    "                \"column\": col,\n",
    "                \"dtype\": dtype,\n",
    "                \"null_pct\": round(null_pct, 3),\n",
    "                \"max_text_len\": max_len,\n",
    "                \"is_unique\": is_unique,\n",
    "                \"all_nonnull\": all_nonnull,\n",
    "                \"pk_candidate\": is_pk,\n",
    "                \"row_count\": row_count,\n",
    "                \"col_count\": col_count\n",
    "            })\n",
    "    else:\n",
    "        print(f\"⚠️ File not found: {name}\")\n",
    "\n",
    "# Export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(DATA_DIR, \"sql_compatibility_summary.csv\"), index=False)\n",
    "\n",
    "print(\"✅ SQL compatibility summary saved to: data/sql_compatibility_summary.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c908e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Repos\\NFL_Draft\\data\\combine_data\\combine_data.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\combine_data\\combine_data_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_picks\\draft_picks.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_picks\\draft_picks_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_values\\draft_values.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_values\\draft_values_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\id_mappings\\id_mappings.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\id_mappings\\id_mappings_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_1999.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_1999_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2000.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2000_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2001.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2001_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2002.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2002_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2003.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2003_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2004.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2004_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2005.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2005_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2006.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2006_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2007.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2007_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2008.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2008_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2009.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2009_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2010.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2010_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2011.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2011_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2012.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2012_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2013.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2013_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2014.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2014_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2015.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2015_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2016.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2016_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2017.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2017_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2018.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2018_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2019.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2019_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2020.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2020_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2021.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2021_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2022.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2022_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2023.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2023_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2024.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2024_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_all_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_data\\seasonal_stats.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_data\\seasonal_stats_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_rosters\\seasonal_rosters.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_rosters\\seasonal_rosters_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\team_info\\team_info.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\team_info\\team_info_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\weekly_data\\weekly_stats.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\weekly_data\\weekly_stats_cleaned.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root = r\"C:\\Repos\\NFL_Draft\\data\"\n",
    "for dirpath, _, filenames in os.walk(root):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            print(os.path.join(dirpath, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6b474ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading combine_data...\n",
      "✅ combine_data: 8,649 rows inserted.\n",
      "🔄 Loading draft_picks...\n",
      "✅ draft_picks: 6,640 rows inserted.\n",
      "🔄 Loading draft_values...\n",
      "✅ draft_values: 262 rows inserted.\n",
      "🔄 Loading id_mappings...\n",
      "✅ id_mappings: 12,023 rows inserted.\n",
      "🔄 Loading seasonal_stats...\n",
      "✅ seasonal_stats: 15,102 rows inserted.\n",
      "🔄 Loading seasonal_rosters...\n",
      "✅ seasonal_rosters: 63,323 rows inserted.\n",
      "🔄 Loading team_info...\n",
      "✅ team_info: 36 rows inserted.\n",
      "🔄 Loading weekly_stats...\n",
      "✅ weekly_stats: 134,470 rows inserted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ---------------------------\n",
    "# 1. SQL Server Connection\n",
    "# ---------------------------\n",
    "conn_str = (\n",
    "    \"mssql+pyodbc://@RAMSEY_BOLTON\\\\SQLEXPRESS/NFL_Analytics\"\n",
    "    \"?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\n",
    ")\n",
    "engine = create_engine(conn_str, fast_executemany=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Cleaning Helpers\n",
    "# ---------------------------\n",
    "def truncate_string_columns(df, max_len=510):\n",
    "    for col in df.select_dtypes(include='object'):\n",
    "        df[col] = df[col].astype(str).str.slice(0, max_len)\n",
    "    return df\n",
    "\n",
    "def clean_and_round_float_columns(df, decimal_places=6):\n",
    "    float_cols = df.select_dtypes(include='number').columns\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').round(decimal_places)\n",
    "    return df\n",
    "\n",
    "def coerce_nulls(df):\n",
    "    return df.where(pd.notnull(df), None)\n",
    "\n",
    "def clean_seasonal_stats(df):\n",
    "    df = truncate_string_columns(df)\n",
    "    df = clean_and_round_float_columns(df, decimal_places=6)\n",
    "    \n",
    "    # Clip float range to avoid SQL precision/scale issues\n",
    "    float_cols = df.select_dtypes(include='number').columns\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].mask((df[col] > 1e5) | (df[col] < -1e5), None)\n",
    "    \n",
    "    return coerce_nulls(df)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. File Paths\n",
    "# ---------------------------\n",
    "parquet_paths = {\n",
    "    \"combine_data\": r\"C:\\Repos\\NFL_Draft\\data\\combine_data\\combine_data_cleaned.parquet\",\n",
    "    \"draft_picks\": r\"C:\\Repos\\NFL_Draft\\data\\draft_picks\\draft_picks_cleaned.parquet\",\n",
    "    \"draft_values\": r\"C:\\Repos\\NFL_Draft\\data\\draft_values\\draft_values_cleaned.parquet\",\n",
    "    \"id_mappings\": r\"C:\\Repos\\NFL_Draft\\data\\id_mappings\\id_mappings_cleaned.parquet\",\n",
    "    \"seasonal_stats\": r\"C:\\Repos\\NFL_Draft\\data\\seasonal_data\\seasonal_stats_cleaned.parquet\",\n",
    "    \"seasonal_rosters\": r\"C:\\Repos\\NFL_Draft\\data\\seasonal_rosters\\seasonal_rosters_cleaned.parquet\",\n",
    "    \"team_info\": r\"C:\\Repos\\NFL_Draft\\data\\team_info\\team_info_cleaned.parquet\",\n",
    "    \"weekly_stats\": r\"C:\\Repos\\NFL_Draft\\data\\weekly_data\\weekly_stats_cleaned.parquet\"\n",
    "}\n",
    "\n",
    "batch_tables = {\"seasonal_stats\", \"seasonal_rosters\", \"weekly_stats\"}\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Load Logic\n",
    "# ---------------------------\n",
    "def load_parquet_to_sql(table_name, file_path, default_chunksize=10000):\n",
    "    print(f\"🔄 Loading {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        if table_name == \"seasonal_stats\":\n",
    "            df = clean_seasonal_stats(df)\n",
    "        else:\n",
    "            df = truncate_string_columns(df)\n",
    "            df = clean_and_round_float_columns(df)\n",
    "            df = coerce_nulls(df)\n",
    "\n",
    "        chunksize = default_chunksize if table_name in batch_tables else None\n",
    "\n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            con=engine,\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            chunksize=chunksize\n",
    "        )\n",
    "\n",
    "        print(f\"✅ {table_name}: {len(df):,} rows inserted.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to insert {table_name} → {e}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Run Loader\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    for table, path in parquet_paths.items():\n",
    "        load_parquet_to_sql(table, path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a58a028c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Check</th>\n",
       "      <th>Table</th>\n",
       "      <th>Column</th>\n",
       "      <th>Passed</th>\n",
       "      <th>Detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>combine_data</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>draft_picks</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>draft_values</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td>player_id</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>team_info</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_name</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>✓ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PK uniqueness</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td>player_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PK uniqueness</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td>3 duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td>player_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_id</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_name</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td>✓ OK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Check             Table     Column  Passed  \\\n",
       "0    Table exists      combine_data               True   \n",
       "1    Table exists       draft_picks               True   \n",
       "2    Table exists      draft_values               True   \n",
       "3    Table exists       id_mappings               True   \n",
       "4   Column exists       id_mappings  player_id   False   \n",
       "5    Table exists         team_info               True   \n",
       "6   Column exists         team_info    team_id    True   \n",
       "7   Column exists         team_info  team_name    True   \n",
       "8    Table exists    seasonal_stats               True   \n",
       "9   Column exists    seasonal_stats  player_id    True   \n",
       "10  Column exists    seasonal_stats     season    True   \n",
       "11   Table exists      weekly_stats               True   \n",
       "12  Column exists      weekly_stats  player_id    True   \n",
       "13  Column exists      weekly_stats     season    True   \n",
       "14   Table exists  seasonal_rosters               True   \n",
       "15  Column exists  seasonal_rosters  player_id    True   \n",
       "16  Column exists  seasonal_rosters    team_id   False   \n",
       "17  PK uniqueness       id_mappings  player_id   False   \n",
       "18  PK uniqueness         team_info    team_id   False   \n",
       "19   FK integrity    seasonal_stats  player_id    True   \n",
       "20   FK integrity  seasonal_rosters  player_id    True   \n",
       "21   FK integrity      weekly_stats  player_id    True   \n",
       "22   FK integrity  seasonal_rosters    team_id   False   \n",
       "23   FK integrity      weekly_stats    team_id   False   \n",
       "24     NULL check       id_mappings  player_id   False   \n",
       "25     NULL check         team_info    team_id    True   \n",
       "26     NULL check         team_info  team_name    True   \n",
       "27     NULL check    seasonal_stats  player_id    True   \n",
       "28     NULL check    seasonal_stats     season    True   \n",
       "29     NULL check      weekly_stats  player_id    True   \n",
       "30     NULL check      weekly_stats     season    True   \n",
       "\n",
       "                                               Detail  \n",
       "0                                             ✓ Found  \n",
       "1                                             ✓ Found  \n",
       "2                                             ✓ Found  \n",
       "3                                             ✓ Found  \n",
       "4                                                      \n",
       "5                                             ✓ Found  \n",
       "6                                                      \n",
       "7                                                      \n",
       "8                                             ✓ Found  \n",
       "9                                                      \n",
       "10                                                     \n",
       "11                                            ✓ Found  \n",
       "12                                                     \n",
       "13                                                     \n",
       "14                                            ✓ Found  \n",
       "15                                                     \n",
       "16                                                     \n",
       "17  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "18                                       3 duplicates  \n",
       "19                                               ✓ OK  \n",
       "20                                               ✓ OK  \n",
       "21                                               ✓ OK  \n",
       "22  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "23  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "24  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "25                                               ✓ OK  \n",
       "26                                               ✓ OK  \n",
       "27                                               ✓ OK  \n",
       "28                                               ✓ OK  \n",
       "29                                               ✓ OK  \n",
       "30                                               ✓ OK  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved verification summary to:\n",
      "C:\\Repos\\NFL_Draft\\data\\database_verification_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Connection ---\n",
    "conn_str = (\n",
    "    \"mssql+pyodbc://@RAMSEY_BOLTON\\\\SQLEXPRESS/NFL_Analytics\"\n",
    "    \"?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\n",
    ")\n",
    "engine = create_engine(conn_str)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# --- Initialize ---\n",
    "results = []\n",
    "\n",
    "def log(check_type, table, column, passed, detail=\"\"):\n",
    "    results.append({\n",
    "        \"Check\": check_type,\n",
    "        \"Table\": table,\n",
    "        \"Column\": column,\n",
    "        \"Passed\": passed,\n",
    "        \"Detail\": detail\n",
    "    })\n",
    "\n",
    "# --- A. Table + Column Existence ---\n",
    "expected_columns = {\n",
    "    \"combine_data\": [],\n",
    "    \"draft_picks\": [],\n",
    "    \"draft_values\": [],\n",
    "    \"id_mappings\": [\"player_id\"],\n",
    "    \"team_info\": [\"team_id\", \"team_name\"],\n",
    "    \"seasonal_stats\": [\"player_id\", \"season\"],\n",
    "    \"weekly_stats\": [\"player_id\", \"season\"],\n",
    "    \"seasonal_rosters\": [\"player_id\", \"team_id\"]\n",
    "}\n",
    "\n",
    "for table, cols in expected_columns.items():\n",
    "    if not inspector.has_table(table):\n",
    "        log(\"Table exists\", table, \"\", False, \"Missing\")\n",
    "        continue\n",
    "    log(\"Table exists\", table, \"\", True, \"✓ Found\")\n",
    "    actual_cols = [col[\"name\"] for col in inspector.get_columns(table)]\n",
    "    for col in cols:\n",
    "        log(\"Column exists\", table, col, col in actual_cols)\n",
    "\n",
    "# --- B. PK Uniqueness (Assumed) ---\n",
    "pk_checks = {\n",
    "    \"id_mappings\": \"player_id\",\n",
    "    \"team_info\": \"team_id\"\n",
    "}\n",
    "\n",
    "for table, col in pk_checks.items():\n",
    "    if inspector.has_table(table):\n",
    "        try:\n",
    "            query = f\"SELECT COUNT(*) AS dupes FROM (SELECT {col} FROM {table} GROUP BY {col} HAVING COUNT(*) > 1) AS sub\"\n",
    "            dupes = pd.read_sql(query, engine).iloc[0][\"dupes\"]\n",
    "            log(\"PK uniqueness\", table, col, dupes == 0, f\"{dupes} duplicates\" if dupes else \"✓ Unique\")\n",
    "        except Exception as e:\n",
    "            log(\"PK uniqueness\", table, col, False, f\"Error: {e}\")\n",
    "\n",
    "# --- C. Foreign Key Relationships ---\n",
    "fk_checks = [\n",
    "    (\"seasonal_stats\", \"player_id\", \"id_mappings\"),\n",
    "    (\"seasonal_rosters\", \"player_id\", \"id_mappings\"),\n",
    "    (\"weekly_stats\", \"player_id\", \"id_mappings\"),\n",
    "    (\"seasonal_rosters\", \"team_id\", \"team_info\"),\n",
    "    (\"weekly_stats\", \"team_id\", \"team_info\"),\n",
    "]\n",
    "\n",
    "for src, col, tgt in fk_checks:\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT COUNT(*) AS missing\n",
    "        FROM {src}\n",
    "        WHERE {col} IS NOT NULL\n",
    "        AND {col} NOT IN (SELECT DISTINCT {col} FROM {tgt})\n",
    "        \"\"\"\n",
    "        missing = pd.read_sql(query, engine).iloc[0][\"missing\"]\n",
    "        log(\"FK integrity\", src, col, missing == 0, f\"{missing} unmatched\" if missing else \"✓ OK\")\n",
    "    except Exception as e:\n",
    "        log(\"FK integrity\", src, col, False, f\"Error: {e}\")\n",
    "\n",
    "# --- D. Null Checks ---\n",
    "required_fields = {\n",
    "    \"id_mappings\": [\"player_id\"],\n",
    "    \"team_info\": [\"team_id\", \"team_name\"],\n",
    "    \"seasonal_stats\": [\"player_id\", \"season\"],\n",
    "    \"weekly_stats\": [\"player_id\", \"season\"]\n",
    "}\n",
    "\n",
    "for table, cols in required_fields.items():\n",
    "    for col in cols:\n",
    "        try:\n",
    "            nulls = pd.read_sql(f\"SELECT COUNT(*) AS n FROM {table} WHERE {col} IS NULL\", engine).iloc[0][\"n\"]\n",
    "            log(\"NULL check\", table, col, nulls == 0, f\"{nulls} NULL(s)\" if nulls else \"✓ OK\")\n",
    "        except Exception as e:\n",
    "            log(\"NULL check\", table, col, False, f\"Error: {e}\")\n",
    "\n",
    "# --- Export + Display ---\n",
    "df_result = pd.DataFrame(results)\n",
    "csv_path = r\"C:\\Repos\\NFL_Draft\\data\\database_verification_summary.csv\"\n",
    "df_result.to_csv(csv_path, index=False)\n",
    "display(df_result)\n",
    "print(f\"\\n✅ Saved verification summary to:\\n{csv_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
