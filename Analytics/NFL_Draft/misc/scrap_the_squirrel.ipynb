{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc6c136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downcasting floats.\n",
      "âœ… Saved: weekly_stats (134,470 rows, 53 columns)\n",
      "âœ… Saved: seasonal_stats (15,102 rows, 58 columns)\n",
      "âœ… Saved: seasonal_rosters (63,323 rows, 37 columns)\n",
      "âœ… Saved: combine_data (8,649 rows, 18 columns)\n",
      "âœ… Saved: draft_picks (6,640 rows, 36 columns)\n",
      "âœ… Saved: team_info (36 rows, 16 columns)\n",
      "âœ… Saved: draft_values (262 rows, 6 columns)\n",
      "âœ… Saved: id_mappings (12,023 rows, 35 columns)\n"
     ]
    }
   ],
   "source": [
    "import nfl_data_py as nfl\n",
    "import pandas as pd\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "base_dir = r\"C:\\Repos\\NFL_Draft\\data\"\n",
    "\n",
    "def save_parquet(df, folder_name, file_name):\n",
    "    \"\"\"Save a DataFrame to Parquet format with automatic type coercion and logging.\"\"\"\n",
    "    try:\n",
    "        # Create folder if needed\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "        # Coerce object-type numeric columns to avoid Arrow errors\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"object\" and df[col].str.isnumeric().sum() > 0:\n",
    "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        file_path = os.path.join(folder_path, f\"{file_name}.parquet\")\n",
    "        df.to_parquet(file_path, index=False)\n",
    "        print(f\"âœ… Saved: {file_name} ({df.shape[0]:,} rows, {df.shape[1]} columns)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to save {file_name}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Define range of seasons\n",
    "years = list(range(1999, 2025))\n",
    "\n",
    "# ----------- Main Extraction -----------\n",
    "\n",
    "try:\n",
    "    weekly_stats = nfl.import_weekly_data(years)\n",
    "    save_parquet(weekly_stats, \"weekly_data\", \"weekly_stats\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading weekly_stats:\", e)\n",
    "\n",
    "try:\n",
    "    seasonal_stats = nfl.import_seasonal_data(years)\n",
    "    save_parquet(seasonal_stats, \"seasonal_data\", \"seasonal_stats\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading seasonal_stats:\", e)\n",
    "\n",
    "try:\n",
    "    seasonal_rosters = nfl.import_seasonal_rosters(years)\n",
    "    save_parquet(seasonal_rosters, \"seasonal_rosters\", \"seasonal_rosters\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading seasonal_rosters:\", e)\n",
    "\n",
    "try:\n",
    "    combine_data = nfl.import_combine_data()\n",
    "    save_parquet(combine_data, \"combine_data\", \"combine_data\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading combine_data:\", e)\n",
    "\n",
    "try:\n",
    "    draft_picks = nfl.import_draft_picks(years)\n",
    "    save_parquet(draft_picks, \"draft_picks\", \"draft_picks\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading draft_picks:\", e)\n",
    "\n",
    "try:\n",
    "    team_info = nfl.import_team_desc()\n",
    "    save_parquet(team_info, \"team_info\", \"team_info\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading team_info:\", e)\n",
    "\n",
    "try:\n",
    "    draft_values = nfl.import_draft_values()\n",
    "    save_parquet(draft_values, \"draft_values\", \"draft_values\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading draft_values:\", e)\n",
    "\n",
    "try:\n",
    "    id_mappings = nfl.import_ids()\n",
    "    save_parquet(id_mappings, \"id_mappings\", \"id_mappings\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error downloading id_mappings:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb62fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Summary saved to parquet_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = r\"C:\\Repos\\NFL_Draft\\data\"\n",
    "\n",
    "def summarize_parquet(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    summary = pd.DataFrame({\n",
    "        \"column\": df.columns,\n",
    "        \"dtype\": [str(df[col].dtype) for col in df.columns],\n",
    "        \"nullable\": [df[col].isnull().any() for col in df.columns],\n",
    "        \"null_pct\": [df[col].isnull().mean() for col in df.columns],\n",
    "        \"unique_vals\": [df[col].nunique(dropna=True) for col in df.columns],\n",
    "        \"example\": [df[col].dropna().iloc[0] if not df[col].dropna().empty else None for col in df.columns],\n",
    "    })\n",
    "    summary.insert(0, \"table\", os.path.basename(path).replace(\".parquet\", \"\"))\n",
    "    return summary\n",
    "\n",
    "# Collect all .parquet files\n",
    "parquet_files = []\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            parquet_files.append(os.path.join(root, file))\n",
    "\n",
    "# Generate and export summary\n",
    "if parquet_files:\n",
    "    summaries = pd.concat([summarize_parquet(f) for f in parquet_files], ignore_index=True)\n",
    "    summaries.to_csv(os.path.join(base_dir, \"parquet_summary.csv\"), index=False)\n",
    "    print(\"âœ… Summary saved to parquet_summary.csv\")\n",
    "else:\n",
    "    print(\"âš ï¸ No .parquet files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa27c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… combine_data cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root to sys.path if running from subfolder ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import COMBINE_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_combine_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce numeric columns\n",
    "    for col in [\"draft_year\", \"draft_round\", \"draft_ovr\", \"jersey_number\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Season to nullable integer\n",
    "    if \"season\" in df.columns:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Clean categorical text columns\n",
    "    for col in [\"draft_team\", \"position\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "combine_path = os.path.join(COMBINE_DIR, \"combine_data.parquet\")\n",
    "combine_cleaned_path = os.path.join(COMBINE_DIR, \"combine_data_cleaned.parquet\")\n",
    "\n",
    "df_combine = pd.read_parquet(combine_path)\n",
    "df_combine_cleaned = clean_combine_data(df_combine)\n",
    "df_combine_cleaned.to_parquet(combine_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… combine_data cleaned and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e271ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… draft_picks cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root to sys.path if running from misc/ ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import DRAFT_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_draft_picks(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce integer-like floats to Int64\n",
    "    for col in [\"draft_round\", \"draft_pick\", \"overall_pick\", \"season\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Strip text fields\n",
    "    for col in [\"player_name\", \"position\", \"team\", \"college\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "draft_path = os.path.join(DRAFT_DIR, \"draft_picks.parquet\")\n",
    "draft_cleaned_path = os.path.join(DRAFT_DIR, \"draft_picks_cleaned.parquet\")\n",
    "\n",
    "df_draft = pd.read_parquet(draft_path)\n",
    "df_draft_cleaned = clean_draft_picks(df_draft)\n",
    "df_draft_cleaned.to_parquet(draft_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… draft_picks cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c70233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… draft_values cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import DRAFT_VALUE_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_draft_values(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Clean placeholder values\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Cast pick to Int64 if it's a float with integer values\n",
    "    if \"pick\" in df.columns:\n",
    "        df[\"pick\"] = pd.to_numeric(df[\"pick\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Ensure chart column is clean string\n",
    "    if \"chart\" in df.columns:\n",
    "        df[\"chart\"] = df[\"chart\"].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "values_path = os.path.join(DRAFT_VALUE_DIR, \"draft_values.parquet\")\n",
    "values_cleaned_path = os.path.join(DRAFT_VALUE_DIR, \"draft_values_cleaned.parquet\")\n",
    "\n",
    "df_values = pd.read_parquet(values_path)\n",
    "df_values_cleaned = clean_draft_values(df_values)\n",
    "df_values_cleaned.to_parquet(values_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… draft_values cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dea8dd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… id_mappings cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import ID_MAP_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_id_mappings(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder values\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Strip all string/object columns\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "id_path = os.path.join(ID_MAP_DIR, \"id_mappings.parquet\")\n",
    "id_cleaned_path = os.path.join(ID_MAP_DIR, \"id_mappings_cleaned.parquet\")\n",
    "\n",
    "df_ids = pd.read_parquet(id_path)\n",
    "df_ids_cleaned = clean_id_mappings(df_ids)\n",
    "df_ids_cleaned.to_parquet(id_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… id_mappings cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b518f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… seasonal_stats cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import SEASONAL_STATS_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_seasonal_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder values\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Convert float-to-int where appropriate\n",
    "    for col in df.select_dtypes(include=\"float64\").columns:\n",
    "        if df[col].dropna().apply(float.is_integer).all():\n",
    "            df[col] = df[col].astype(\"Int64\")\n",
    "\n",
    "    # Clean string columns\n",
    "    for col in [\"player_id\", \"player_name\", \"team\", \"position\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Coerce season to Int64\n",
    "    if \"season\" in df.columns:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "seasonal_path = os.path.join(SEASONAL_STATS_DIR, \"seasonal_stats.parquet\")\n",
    "seasonal_cleaned_path = os.path.join(SEASONAL_STATS_DIR, \"seasonal_stats_cleaned.parquet\")\n",
    "\n",
    "df_seasonal = pd.read_parquet(seasonal_path)\n",
    "df_seasonal_cleaned = clean_seasonal_data(df_seasonal)\n",
    "df_seasonal_cleaned.to_parquet(seasonal_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… seasonal_stats cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29f38bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… seasonal_rosters cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import ROSTERS_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_seasonal_rosters(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce numeric columns\n",
    "    for col in [\"jersey_number\", \"height\", \"weight\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Parse birth_date safely\n",
    "    if \"birth_date\" in df.columns:\n",
    "        df[\"birth_date\"] = pd.to_datetime(df[\"birth_date\"], errors=\"coerce\")\n",
    "\n",
    "    # Clean categorical/text fields\n",
    "    for col in [\"player_name\", \"position\", \"team\", \"status\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Coerce season\n",
    "    if \"season\" in df.columns:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "rosters_path = os.path.join(ROSTERS_DIR, \"seasonal_rosters.parquet\")\n",
    "rosters_cleaned_path = os.path.join(ROSTERS_DIR, \"seasonal_rosters_cleaned.parquet\")\n",
    "\n",
    "df_rosters = pd.read_parquet(rosters_path)\n",
    "df_rosters_cleaned = clean_seasonal_rosters(df_rosters)\n",
    "df_rosters_cleaned.to_parquet(rosters_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… seasonal_rosters cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d43717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… team_info cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import TEAM_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_team_info(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Clean text fields\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    # Coerce numeric columns if applicable\n",
    "    for col in [\"season\", \"team_id\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "team_path = os.path.join(TEAM_DIR, \"team_info.parquet\")\n",
    "team_cleaned_path = os.path.join(TEAM_DIR, \"team_info_cleaned.parquet\")\n",
    "\n",
    "df_team = pd.read_parquet(team_path)\n",
    "df_team_cleaned = clean_team_info(df_team)\n",
    "df_team_cleaned.to_parquet(team_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… team_info cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b982843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… weekly_stats cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root if needed ---\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from config import WEEKLY_STATS_DIR\n",
    "\n",
    "\n",
    "# --- Cleaning Function ---\n",
    "def clean_weekly_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Normalize column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "    # Replace placeholder strings\n",
    "    df.replace([\"--\", \"N/A\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "    # Coerce float-to-int where appropriate\n",
    "    for col in df.select_dtypes(include=\"float64\").columns:\n",
    "        if df[col].dropna().apply(float.is_integer).all():\n",
    "            df[col] = df[col].astype(\"Int64\")\n",
    "\n",
    "    # Ensure season/week are integers\n",
    "    for col in [\"season\", \"week\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Clean text columns\n",
    "    for col in [\"player_id\", \"player_name\", \"team\", \"opponent\", \"position\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Load, Clean, Save ---\n",
    "weekly_path = os.path.join(WEEKLY_STATS_DIR, \"weekly_stats.parquet\")\n",
    "weekly_cleaned_path = os.path.join(WEEKLY_STATS_DIR, \"weekly_stats_cleaned.parquet\")\n",
    "\n",
    "df_weekly = pd.read_parquet(weekly_path)\n",
    "df_weekly_cleaned = clean_weekly_data(df_weekly)\n",
    "df_weekly_cleaned.to_parquet(weekly_cleaned_path, index=False)\n",
    "\n",
    "print(\"âœ… weekly_stats cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a17ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning 1999...\n",
      "ðŸ§¹ Cleaning 2000...\n",
      "ðŸ§¹ Cleaning 2001...\n",
      "ðŸ§¹ Cleaning 2002...\n",
      "ðŸ§¹ Cleaning 2003...\n",
      "ðŸ§¹ Cleaning 2004...\n",
      "ðŸ§¹ Cleaning 2005...\n",
      "ðŸ§¹ Cleaning 2006...\n",
      "ðŸ§¹ Cleaning 2007...\n",
      "ðŸ§¹ Cleaning 2008...\n",
      "ðŸ§¹ Cleaning 2009...\n",
      "ðŸ§¹ Cleaning 2010...\n",
      "ðŸ§¹ Cleaning 2011...\n",
      "ðŸ§¹ Cleaning 2012...\n",
      "ðŸ§¹ Cleaning 2013...\n",
      "ðŸ§¹ Cleaning 2014...\n",
      "ðŸ§¹ Cleaning 2015...\n",
      "ðŸ§¹ Cleaning 2016...\n",
      "ðŸ§¹ Cleaning 2017...\n",
      "ðŸ§¹ Cleaning 2018...\n",
      "ðŸ§¹ Cleaning 2019...\n",
      "ðŸ§¹ Cleaning 2020...\n",
      "ðŸ§¹ Cleaning 2021...\n",
      "ðŸ§¹ Cleaning 2022...\n",
      "ðŸ§¹ Cleaning 2023...\n",
      "ðŸ§¹ Cleaning 2024...\n",
      "âœ… All seasons cleaned and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replace this with your actual import if using config.py\n",
    "from config import PBP_DIR\n",
    "\n",
    "# Step 1: Gather all .parquet files\n",
    "pbp_files = sorted([\n",
    "    os.path.join(PBP_DIR, f)\n",
    "    for f in os.listdir(PBP_DIR)\n",
    "    if f.endswith(\".parquet\")\n",
    "])\n",
    "\n",
    "# Step 2: Build a unified column set\n",
    "column_union = set()\n",
    "for path in pbp_files:\n",
    "    df = pd.read_parquet(path)\n",
    "    column_union.update(df.columns)\n",
    "column_union = sorted(column_union)\n",
    "\n",
    "# Step 3: Define cleaner\n",
    "def clean_pbp(df):\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "    df.replace([\"--\", \"N/A\", \"\", \"NaN\"], pd.NA, inplace=True)\n",
    "\n",
    "    for col in column_union:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    numeric_cols = [\"yards_gained\", \"epa\", \"down\", \"quarter\", \"game_seconds_remaining\"]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    for col in [\"game_id\", \"play_id\", \"posteam\", \"defteam\", \"play_type\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return df[column_union]\n",
    "\n",
    "# Step 4: Clean and save each season\n",
    "cleaned_dfs = []\n",
    "for path in pbp_files:\n",
    "    season_year = os.path.basename(path).split(\"_\")[1].split(\".\")[0]\n",
    "    print(f\"ðŸ§¹ Cleaning {season_year}...\")\n",
    "\n",
    "    df_raw = pd.read_parquet(path)\n",
    "    df_cleaned = clean_pbp(df_raw)\n",
    "    df_cleaned[\"season\"] = int(season_year)\n",
    "\n",
    "    cleaned_path = os.path.join(PBP_DIR, f\"pbp_{season_year}_cleaned.parquet\")\n",
    "    df_cleaned.to_parquet(cleaned_path, index=False)\n",
    "    cleaned_dfs.append(df_cleaned)\n",
    "\n",
    "# Step 5: Save full combined file (optional)\n",
    "df_all = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "df_all.to_parquet(os.path.join(PBP_DIR, \"pbp_all_cleaned.parquet\"), index=False)\n",
    "\n",
    "print(\"âœ… All seasons cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e66bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SQL compatibility summary saved to: data/sql_compatibility_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from config import (\n",
    "    COMBINE_DIR, DRAFT_DIR, DRAFT_VALUE_DIR, ID_MAP_DIR,\n",
    "    SEASONAL_STATS_DIR, ROSTERS_DIR, TEAM_DIR, WEEKLY_STATS_DIR, PBP_DIR, DATA_DIR\n",
    ")\n",
    "\n",
    "# Cleaned files to check\n",
    "cleaned_files = {\n",
    "    \"combine_data\": os.path.join(COMBINE_DIR, \"combine_data_cleaned.parquet\"),\n",
    "    \"draft_picks\": os.path.join(DRAFT_DIR, \"draft_picks_cleaned.parquet\"),\n",
    "    \"draft_values\": os.path.join(DRAFT_VALUE_DIR, \"draft_values_cleaned.parquet\"),\n",
    "    \"id_mappings\": os.path.join(ID_MAP_DIR, \"id_mappings_cleaned.parquet\"),\n",
    "    \"seasonal_stats\": os.path.join(SEASONAL_STATS_DIR, \"seasonal_stats_cleaned.parquet\"),\n",
    "    \"seasonal_rosters\": os.path.join(ROSTERS_DIR, \"seasonal_rosters_cleaned.parquet\"),\n",
    "    \"team_info\": os.path.join(TEAM_DIR, \"team_info_cleaned.parquet\"),\n",
    "    \"weekly_stats\": os.path.join(WEEKLY_STATS_DIR, \"weekly_stats_cleaned.parquet\"),\n",
    "    \"pbp_all\": os.path.join(PBP_DIR, \"pbp_all_cleaned.parquet\")\n",
    "}\n",
    "\n",
    "# Run checks\n",
    "results = []\n",
    "\n",
    "for name, path in cleaned_files.items():\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_parquet(path)\n",
    "        row_count = df.shape[0]\n",
    "        col_count = df.shape[1]\n",
    "\n",
    "        for col in df.columns:\n",
    "            dtype = str(df[col].dtype)\n",
    "            null_pct = df[col].isnull().mean()\n",
    "            is_unique = df[col].is_unique\n",
    "            all_nonnull = df[col].notnull().all()\n",
    "            is_pk = is_unique and all_nonnull\n",
    "\n",
    "            max_len = (\n",
    "                df[col].astype(str).str.len().max()\n",
    "                if df[col].dtype == \"object\" else None\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"table\": name,\n",
    "                \"column\": col,\n",
    "                \"dtype\": dtype,\n",
    "                \"null_pct\": round(null_pct, 3),\n",
    "                \"max_text_len\": max_len,\n",
    "                \"is_unique\": is_unique,\n",
    "                \"all_nonnull\": all_nonnull,\n",
    "                \"pk_candidate\": is_pk,\n",
    "                \"row_count\": row_count,\n",
    "                \"col_count\": col_count\n",
    "            })\n",
    "    else:\n",
    "        print(f\"âš ï¸ File not found: {name}\")\n",
    "\n",
    "# Export\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(DATA_DIR, \"sql_compatibility_summary.csv\"), index=False)\n",
    "\n",
    "print(\"âœ… SQL compatibility summary saved to: data/sql_compatibility_summary.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c908e375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Repos\\NFL_Draft\\data\\combine_data\\combine_data.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\combine_data\\combine_data_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_picks\\draft_picks.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_picks\\draft_picks_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_values\\draft_values.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\draft_values\\draft_values_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\id_mappings\\id_mappings.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\id_mappings\\id_mappings_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_1999.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_1999_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2000.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2000_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2001.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2001_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2002.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2002_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2003.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2003_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2004.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2004_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2005.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2005_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2006.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2006_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2007.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2007_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2008.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2008_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2009.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2009_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2010.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2010_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2011.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2011_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2012.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2012_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2013.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2013_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2014.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2014_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2015.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2015_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2016.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2016_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2017.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2017_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2018.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2018_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2019.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2019_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2020.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2020_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2021.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2021_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2022.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2022_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2023.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2023_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2024.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_2024_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\pbp_by_season\\pbp_all_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_data\\seasonal_stats.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_data\\seasonal_stats_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_rosters\\seasonal_rosters.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\seasonal_rosters\\seasonal_rosters_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\team_info\\team_info.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\team_info\\team_info_cleaned.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\weekly_data\\weekly_stats.parquet\n",
      "C:\\Repos\\NFL_Draft\\data\\weekly_data\\weekly_stats_cleaned.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root = r\"C:\\Repos\\NFL_Draft\\data\"\n",
    "for dirpath, _, filenames in os.walk(root):\n",
    "    for file in filenames:\n",
    "        if file.endswith(\".parquet\"):\n",
    "            print(os.path.join(dirpath, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6b474ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading combine_data...\n",
      "âœ… combine_data: 8,649 rows inserted.\n",
      "ðŸ”„ Loading draft_picks...\n",
      "âœ… draft_picks: 6,640 rows inserted.\n",
      "ðŸ”„ Loading draft_values...\n",
      "âœ… draft_values: 262 rows inserted.\n",
      "ðŸ”„ Loading id_mappings...\n",
      "âœ… id_mappings: 12,023 rows inserted.\n",
      "ðŸ”„ Loading seasonal_stats...\n",
      "âœ… seasonal_stats: 15,102 rows inserted.\n",
      "ðŸ”„ Loading seasonal_rosters...\n",
      "âœ… seasonal_rosters: 63,323 rows inserted.\n",
      "ðŸ”„ Loading team_info...\n",
      "âœ… team_info: 36 rows inserted.\n",
      "ðŸ”„ Loading weekly_stats...\n",
      "âœ… weekly_stats: 134,470 rows inserted.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# ---------------------------\n",
    "# 1. SQL Server Connection\n",
    "# ---------------------------\n",
    "conn_str = (\n",
    "    \"mssql+pyodbc://@RAMSEY_BOLTON\\\\SQLEXPRESS/NFL_Analytics\"\n",
    "    \"?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\n",
    ")\n",
    "engine = create_engine(conn_str, fast_executemany=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Cleaning Helpers\n",
    "# ---------------------------\n",
    "def truncate_string_columns(df, max_len=510):\n",
    "    for col in df.select_dtypes(include='object'):\n",
    "        df[col] = df[col].astype(str).str.slice(0, max_len)\n",
    "    return df\n",
    "\n",
    "def clean_and_round_float_columns(df, decimal_places=6):\n",
    "    float_cols = df.select_dtypes(include='number').columns\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').round(decimal_places)\n",
    "    return df\n",
    "\n",
    "def coerce_nulls(df):\n",
    "    return df.where(pd.notnull(df), None)\n",
    "\n",
    "def clean_seasonal_stats(df):\n",
    "    df = truncate_string_columns(df)\n",
    "    df = clean_and_round_float_columns(df, decimal_places=6)\n",
    "    \n",
    "    # Clip float range to avoid SQL precision/scale issues\n",
    "    float_cols = df.select_dtypes(include='number').columns\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].mask((df[col] > 1e5) | (df[col] < -1e5), None)\n",
    "    \n",
    "    return coerce_nulls(df)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. File Paths\n",
    "# ---------------------------\n",
    "parquet_paths = {\n",
    "    \"combine_data\": r\"C:\\Repos\\NFL_Draft\\data\\combine_data\\combine_data_cleaned.parquet\",\n",
    "    \"draft_picks\": r\"C:\\Repos\\NFL_Draft\\data\\draft_picks\\draft_picks_cleaned.parquet\",\n",
    "    \"draft_values\": r\"C:\\Repos\\NFL_Draft\\data\\draft_values\\draft_values_cleaned.parquet\",\n",
    "    \"id_mappings\": r\"C:\\Repos\\NFL_Draft\\data\\id_mappings\\id_mappings_cleaned.parquet\",\n",
    "    \"seasonal_stats\": r\"C:\\Repos\\NFL_Draft\\data\\seasonal_data\\seasonal_stats_cleaned.parquet\",\n",
    "    \"seasonal_rosters\": r\"C:\\Repos\\NFL_Draft\\data\\seasonal_rosters\\seasonal_rosters_cleaned.parquet\",\n",
    "    \"team_info\": r\"C:\\Repos\\NFL_Draft\\data\\team_info\\team_info_cleaned.parquet\",\n",
    "    \"weekly_stats\": r\"C:\\Repos\\NFL_Draft\\data\\weekly_data\\weekly_stats_cleaned.parquet\"\n",
    "}\n",
    "\n",
    "batch_tables = {\"seasonal_stats\", \"seasonal_rosters\", \"weekly_stats\"}\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Load Logic\n",
    "# ---------------------------\n",
    "def load_parquet_to_sql(table_name, file_path, default_chunksize=10000):\n",
    "    print(f\"ðŸ”„ Loading {table_name}...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        if table_name == \"seasonal_stats\":\n",
    "            df = clean_seasonal_stats(df)\n",
    "        else:\n",
    "            df = truncate_string_columns(df)\n",
    "            df = clean_and_round_float_columns(df)\n",
    "            df = coerce_nulls(df)\n",
    "\n",
    "        chunksize = default_chunksize if table_name in batch_tables else None\n",
    "\n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            con=engine,\n",
    "            if_exists=\"append\",\n",
    "            index=False,\n",
    "            chunksize=chunksize\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… {table_name}: {len(df):,} rows inserted.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to insert {table_name} â†’ {e}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Run Loader\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    for table, path in parquet_paths.items():\n",
    "        load_parquet_to_sql(table, path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a58a028c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Check</th>\n",
       "      <th>Table</th>\n",
       "      <th>Column</th>\n",
       "      <th>Passed</th>\n",
       "      <th>Detail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>combine_data</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>draft_picks</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>draft_values</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td>player_id</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>team_info</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_name</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Table exists</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ Found</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Column exists</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PK uniqueness</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td>player_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PK uniqueness</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td>3 duplicates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>seasonal_rosters</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>FK integrity</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>team_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>id_mappings</td>\n",
       "      <td>player_id</td>\n",
       "      <td>False</td>\n",
       "      <td>Error: (pyodbc.ProgrammingError) ('42S22', \"[4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_id</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>team_info</td>\n",
       "      <td>team_name</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>seasonal_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>player_id</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NULL check</td>\n",
       "      <td>weekly_stats</td>\n",
       "      <td>season</td>\n",
       "      <td>True</td>\n",
       "      <td>âœ“ OK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Check             Table     Column  Passed  \\\n",
       "0    Table exists      combine_data               True   \n",
       "1    Table exists       draft_picks               True   \n",
       "2    Table exists      draft_values               True   \n",
       "3    Table exists       id_mappings               True   \n",
       "4   Column exists       id_mappings  player_id   False   \n",
       "5    Table exists         team_info               True   \n",
       "6   Column exists         team_info    team_id    True   \n",
       "7   Column exists         team_info  team_name    True   \n",
       "8    Table exists    seasonal_stats               True   \n",
       "9   Column exists    seasonal_stats  player_id    True   \n",
       "10  Column exists    seasonal_stats     season    True   \n",
       "11   Table exists      weekly_stats               True   \n",
       "12  Column exists      weekly_stats  player_id    True   \n",
       "13  Column exists      weekly_stats     season    True   \n",
       "14   Table exists  seasonal_rosters               True   \n",
       "15  Column exists  seasonal_rosters  player_id    True   \n",
       "16  Column exists  seasonal_rosters    team_id   False   \n",
       "17  PK uniqueness       id_mappings  player_id   False   \n",
       "18  PK uniqueness         team_info    team_id   False   \n",
       "19   FK integrity    seasonal_stats  player_id    True   \n",
       "20   FK integrity  seasonal_rosters  player_id    True   \n",
       "21   FK integrity      weekly_stats  player_id    True   \n",
       "22   FK integrity  seasonal_rosters    team_id   False   \n",
       "23   FK integrity      weekly_stats    team_id   False   \n",
       "24     NULL check       id_mappings  player_id   False   \n",
       "25     NULL check         team_info    team_id    True   \n",
       "26     NULL check         team_info  team_name    True   \n",
       "27     NULL check    seasonal_stats  player_id    True   \n",
       "28     NULL check    seasonal_stats     season    True   \n",
       "29     NULL check      weekly_stats  player_id    True   \n",
       "30     NULL check      weekly_stats     season    True   \n",
       "\n",
       "                                               Detail  \n",
       "0                                             âœ“ Found  \n",
       "1                                             âœ“ Found  \n",
       "2                                             âœ“ Found  \n",
       "3                                             âœ“ Found  \n",
       "4                                                      \n",
       "5                                             âœ“ Found  \n",
       "6                                                      \n",
       "7                                                      \n",
       "8                                             âœ“ Found  \n",
       "9                                                      \n",
       "10                                                     \n",
       "11                                            âœ“ Found  \n",
       "12                                                     \n",
       "13                                                     \n",
       "14                                            âœ“ Found  \n",
       "15                                                     \n",
       "16                                                     \n",
       "17  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "18                                       3 duplicates  \n",
       "19                                               âœ“ OK  \n",
       "20                                               âœ“ OK  \n",
       "21                                               âœ“ OK  \n",
       "22  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "23  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "24  Error: (pyodbc.ProgrammingError) ('42S22', \"[4...  \n",
       "25                                               âœ“ OK  \n",
       "26                                               âœ“ OK  \n",
       "27                                               âœ“ OK  \n",
       "28                                               âœ“ OK  \n",
       "29                                               âœ“ OK  \n",
       "30                                               âœ“ OK  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Saved verification summary to:\n",
      "C:\\Repos\\NFL_Draft\\data\\database_verification_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Connection ---\n",
    "conn_str = (\n",
    "    \"mssql+pyodbc://@RAMSEY_BOLTON\\\\SQLEXPRESS/NFL_Analytics\"\n",
    "    \"?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server\"\n",
    ")\n",
    "engine = create_engine(conn_str)\n",
    "inspector = inspect(engine)\n",
    "\n",
    "# --- Initialize ---\n",
    "results = []\n",
    "\n",
    "def log(check_type, table, column, passed, detail=\"\"):\n",
    "    results.append({\n",
    "        \"Check\": check_type,\n",
    "        \"Table\": table,\n",
    "        \"Column\": column,\n",
    "        \"Passed\": passed,\n",
    "        \"Detail\": detail\n",
    "    })\n",
    "\n",
    "# --- A. Table + Column Existence ---\n",
    "expected_columns = {\n",
    "    \"combine_data\": [],\n",
    "    \"draft_picks\": [],\n",
    "    \"draft_values\": [],\n",
    "    \"id_mappings\": [\"player_id\"],\n",
    "    \"team_info\": [\"team_id\", \"team_name\"],\n",
    "    \"seasonal_stats\": [\"player_id\", \"season\"],\n",
    "    \"weekly_stats\": [\"player_id\", \"season\"],\n",
    "    \"seasonal_rosters\": [\"player_id\", \"team_id\"]\n",
    "}\n",
    "\n",
    "for table, cols in expected_columns.items():\n",
    "    if not inspector.has_table(table):\n",
    "        log(\"Table exists\", table, \"\", False, \"Missing\")\n",
    "        continue\n",
    "    log(\"Table exists\", table, \"\", True, \"âœ“ Found\")\n",
    "    actual_cols = [col[\"name\"] for col in inspector.get_columns(table)]\n",
    "    for col in cols:\n",
    "        log(\"Column exists\", table, col, col in actual_cols)\n",
    "\n",
    "# --- B. PK Uniqueness (Assumed) ---\n",
    "pk_checks = {\n",
    "    \"id_mappings\": \"player_id\",\n",
    "    \"team_info\": \"team_id\"\n",
    "}\n",
    "\n",
    "for table, col in pk_checks.items():\n",
    "    if inspector.has_table(table):\n",
    "        try:\n",
    "            query = f\"SELECT COUNT(*) AS dupes FROM (SELECT {col} FROM {table} GROUP BY {col} HAVING COUNT(*) > 1) AS sub\"\n",
    "            dupes = pd.read_sql(query, engine).iloc[0][\"dupes\"]\n",
    "            log(\"PK uniqueness\", table, col, dupes == 0, f\"{dupes} duplicates\" if dupes else \"âœ“ Unique\")\n",
    "        except Exception as e:\n",
    "            log(\"PK uniqueness\", table, col, False, f\"Error: {e}\")\n",
    "\n",
    "# --- C. Foreign Key Relationships ---\n",
    "fk_checks = [\n",
    "    (\"seasonal_stats\", \"player_id\", \"id_mappings\"),\n",
    "    (\"seasonal_rosters\", \"player_id\", \"id_mappings\"),\n",
    "    (\"weekly_stats\", \"player_id\", \"id_mappings\"),\n",
    "    (\"seasonal_rosters\", \"team_id\", \"team_info\"),\n",
    "    (\"weekly_stats\", \"team_id\", \"team_info\"),\n",
    "]\n",
    "\n",
    "for src, col, tgt in fk_checks:\n",
    "    try:\n",
    "        query = f\"\"\"\n",
    "        SELECT COUNT(*) AS missing\n",
    "        FROM {src}\n",
    "        WHERE {col} IS NOT NULL\n",
    "        AND {col} NOT IN (SELECT DISTINCT {col} FROM {tgt})\n",
    "        \"\"\"\n",
    "        missing = pd.read_sql(query, engine).iloc[0][\"missing\"]\n",
    "        log(\"FK integrity\", src, col, missing == 0, f\"{missing} unmatched\" if missing else \"âœ“ OK\")\n",
    "    except Exception as e:\n",
    "        log(\"FK integrity\", src, col, False, f\"Error: {e}\")\n",
    "\n",
    "# --- D. Null Checks ---\n",
    "required_fields = {\n",
    "    \"id_mappings\": [\"player_id\"],\n",
    "    \"team_info\": [\"team_id\", \"team_name\"],\n",
    "    \"seasonal_stats\": [\"player_id\", \"season\"],\n",
    "    \"weekly_stats\": [\"player_id\", \"season\"]\n",
    "}\n",
    "\n",
    "for table, cols in required_fields.items():\n",
    "    for col in cols:\n",
    "        try:\n",
    "            nulls = pd.read_sql(f\"SELECT COUNT(*) AS n FROM {table} WHERE {col} IS NULL\", engine).iloc[0][\"n\"]\n",
    "            log(\"NULL check\", table, col, nulls == 0, f\"{nulls} NULL(s)\" if nulls else \"âœ“ OK\")\n",
    "        except Exception as e:\n",
    "            log(\"NULL check\", table, col, False, f\"Error: {e}\")\n",
    "\n",
    "# --- Export + Display ---\n",
    "df_result = pd.DataFrame(results)\n",
    "csv_path = r\"C:\\Repos\\NFL_Draft\\data\\database_verification_summary.csv\"\n",
    "df_result.to_csv(csv_path, index=False)\n",
    "display(df_result)\n",
    "print(f\"\\nâœ… Saved verification summary to:\\n{csv_path}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
